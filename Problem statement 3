
/**
takes multiple HDFS directoty paths and displays recursively the files and contents of its sub directories
**/


import java.io.*;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.fs.FileStatus;

public class recursiveListMultiple {
	public static void main(String[] args) {
		if (args.length != 2) {
			System.out.println("Pass two arguments");
			System.exit(1);
		}
		
		for (String val : args){
			Path path = new Path(val);
			readFileRecursive(path);
		}
		
		
	}
	
	private static void readFileRecursive(Path path)
	{
	//FileStatus[] fileStatus=fileSystem.listStatus(path);
		Configuration conf = new Configuration();
		FileSystem fileSystem;
		try {
			fileSystem = FileSystem.get(path.toUri(), conf);
			FileStatus[] fileStatus=fileSystem.listStatus(path);
			for (FileStatus fStat : fileStatus) 
			{
				if (fStat.isDirectory()) {
					System.out.println("Directory: " + fStat.getPath());
					readFileRecursive(fStat.getPath());
				}
				else if (fStat.isFile()) {
					System.out.println("File: " + fStat.getPath());
				}
			}
		} catch (IOException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}
	}
}
